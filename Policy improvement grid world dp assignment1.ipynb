{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Iteration \n",
    "Policy iteration means getting an optimal policy by improving the basic policy by iteratively evaluating the state value function for each state.\n",
    "It has two part \n",
    "\n",
    "1- policy evaluation:- In this we evaluate value function of each action in a state by using the policy.\n",
    "\n",
    "2- Policy improvement:- Iteratively evaluating state value function for each state to get the optimal policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the parameters\n",
    "gamma = 1\n",
    "rewardSize = -1\n",
    "gridSize = 4\n",
    "terminationStates = [[0, 0], [gridSize -1, gridSize -1]]\n",
    "actions = [[-1, 0], [1, 0], [0, 1], [0, -1]] \n",
    "numIterations = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actionRewardFunction(initialPosition, action):\n",
    "    if initialPosition in terminationStates:\n",
    "        return initialPosition, 0\n",
    "    \n",
    "    reward = rewardSize\n",
    "    finalPosition = np.array(initialPosition) + np.array(action)\n",
    "    \n",
    "    \n",
    "    if -1 in finalPosition or 4 in finalPosition:\n",
    "        finalPosition = initialPosition\n",
    "        \n",
    "    return finalPosition, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridWorld = np.zeros((gridSize, gridSize))\n",
    "states = [[i, j] for i in range(gridSize) for j in range(gridSize)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1\n",
      "[[ 0. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1.  0.]]\n",
      "\n",
      "Iteration 2\n",
      "[[ 0.   -1.75 -2.   -2.  ]\n",
      " [-1.75 -2.   -2.   -2.  ]\n",
      " [-2.   -2.   -2.   -1.75]\n",
      " [-2.   -2.   -1.75  0.  ]]\n",
      "\n",
      "Iteration 3\n",
      "[[ 0.     -2.4375 -2.9375 -3.    ]\n",
      " [-2.4375 -2.875  -3.     -2.9375]\n",
      " [-2.9375 -3.     -2.875  -2.4375]\n",
      " [-3.     -2.9375 -2.4375  0.    ]]\n",
      "\n",
      "Iteration 10\n",
      "[[ 0.         -6.13796997 -8.35235596 -8.96731567]\n",
      " [-6.13796997 -7.73739624 -8.42782593 -8.35235596]\n",
      " [-8.35235596 -8.42782593 -7.73739624 -6.13796997]\n",
      " [-8.96731567 -8.35235596 -6.13796997  0.        ]]\n",
      "\n",
      "Iteration 100\n",
      "[[  0.         -13.94260509 -19.91495107 -21.90482522]\n",
      " [-13.94260509 -17.92507693 -19.91551999 -19.91495107]\n",
      " [-19.91495107 -19.91551999 -17.92507693 -13.94260509]\n",
      " [-21.90482522 -19.91495107 -13.94260509   0.        ]]\n",
      "\n",
      "Iteration 1000\n",
      "[[  0. -14. -20. -22.]\n",
      " [-14. -18. -20. -20.]\n",
      " [-20. -20. -18. -14.]\n",
      " [-22. -20. -14.   0.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Policy evaluation\n",
    "deltas = []\n",
    "for i in range(numIterations):\n",
    "    copygridWorld = np.copy(gridWorld)\n",
    "    deltaState = []\n",
    "    \n",
    "    for state in states:\n",
    "        weightedRewards = 0\n",
    "        for action in actions:\n",
    "            finalPosition, reward = actionRewardFunction(state, action)\n",
    "            weightedRewards += (1/len(actions))*(reward + gamma*(gridWorld[finalPosition[0], finalPosition[1]]))\n",
    "        deltaState.append(np.abs(copygridWorld[state[0], state[1]] - weightedRewards))\n",
    "        copygridWorld[state[0], state[1]] = weightedRewards\n",
    "    deltas.append(deltaState)        \n",
    "    gridWorld = copygridWorld\n",
    "            \n",
    "    if i in [0, 1, 2, 9, 99, numIterations-1]:\n",
    "        print(\"Iteration {}\".format(i+1))\n",
    "        print(gridWorld)\n",
    "        print(\"\")\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using policy iteration methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rat, cheese, cat\n",
    "\n",
    "position of rat in starting is (0,0) cat1 is (1,1) and cat2 is (2,4) and position of the cheese is (4,4).\n",
    "rewards \n",
    "for cat -1.5\n",
    "\n",
    "for cheese 5\n",
    "\n",
    "every step -1\n",
    "\n",
    "initial state -1\n",
    "\n",
    "A grid world of 5*5 and in each state actions are up, down, left, right.\n",
    "\n",
    "using random policy so probability of each action is 0.25.\n",
    "\n",
    "apply policy iteration.\n",
    "\n",
    "1 - policy evaluation\n",
    "\n",
    "2 - policy improvement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 1 #discount factor\n",
    "rewardGoal = 5\n",
    "rewardCurrent = -1.5\n",
    "rewardValue = -1\n",
    "gridSize = 5\n",
    "terminationState1 = [[0,0]]\n",
    "terminationState2 = [[1,1], [2,4]]\n",
    "terminationState3 = [[gridSize-1, gridSize-1]] \n",
    "#initialState = [4,0]\n",
    "actions = [[-1,0], [1,0], [0,1], [0,-1]] #up, down, right, left.\n",
    "numIterations = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actionRewardFunction(initialState, action):\n",
    "    if initialState in terminationState1:\n",
    "        return initialState, -1.5\n",
    "    \n",
    "    \n",
    "    \n",
    "    if initialState in terminationState2:\n",
    "        return initialState, -2.5\n",
    "    \n",
    "    if initialState in terminationState3:\n",
    "        return initialState, 5\n",
    "    \n",
    "    reward = rewardValue\n",
    "    finalPosition = np.array(initialState) + np.array(action)\n",
    "    \n",
    "    if -1 in finalPosition or 5 in finalPosition:\n",
    "        finalPosition = initialPosition\n",
    "        \n",
    "    return finalPosition, reward\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridWorld = np.zeros((gridSize, gridSize))\n",
    "states = [[i,j] for i in range(gridSize) for j in range(gridSize)]\n",
    "gridWorld\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1\n",
      "[[-1.5 -1.  -1.  -1.  -1. ]\n",
      " [-1.  -2.5 -1.  -1.  -1. ]\n",
      " [-1.  -1.  -1.  -1.  -2.5]\n",
      " [-1.  -1.  -1.  -1.  -1. ]\n",
      " [-1.  -1.  -1.  -1.   5. ]]\n",
      "\n",
      "Iteration 2\n",
      "[[-3.    -2.625 -2.125 -2.125 -2.25 ]\n",
      " [-2.625 -5.    -2.375 -2.    -2.5  ]\n",
      " [-2.125 -2.375 -2.    -2.375 -5.   ]\n",
      " [-2.125 -2.    -2.    -2.    -1.   ]\n",
      " [-2.25  -2.125 -2.125 -0.625 10.   ]]\n",
      "\n",
      "Iteration 3\n",
      "[[-4.5     -4.28125 -3.53125 -3.34375 -3.65625]\n",
      " [-4.28125 -7.5     -3.78125 -3.34375 -4.0625 ]\n",
      " [-3.53125 -3.78125 -3.28125 -3.75    -7.5    ]\n",
      " [-3.34375 -3.15625 -3.03125 -2.5     -1.     ]\n",
      " [-3.5625  -3.34375 -2.9375  -0.28125 15.     ]]\n",
      "\n",
      "Iteration 10\n",
      "[[-1.50000000e+01 -1.66455097e+01 -1.47559910e+01 -1.40675640e+01\n",
      "  -1.45156422e+01]\n",
      " [-1.65937004e+01 -2.50000000e+01 -1.60497665e+01 -1.44545593e+01\n",
      "  -1.64220543e+01]\n",
      " [-1.44929543e+01 -1.55455990e+01 -1.33647327e+01 -1.40742321e+01\n",
      "  -2.50000000e+01]\n",
      " [-1.33275814e+01 -1.23315506e+01 -9.92645645e+00 -5.63558960e+00\n",
      "  -4.49752808e-02]\n",
      " [-1.34057350e+01 -1.20384636e+01 -8.52122116e+00  3.65044212e+00\n",
      "   5.00000000e+01]]\n",
      "\n",
      "Iteration 100\n",
      "[[-150.         -178.73704868 -168.15368061 -161.55314479 -160.22269642]\n",
      " [-177.30151291 -250.         -185.14647264 -170.39585554 -181.81609612]\n",
      " [-162.35003433 -176.14793623 -155.55898967 -156.00592337 -250.        ]\n",
      " [-148.52661886 -139.82824421 -107.26600918  -50.38525104   11.2785006 ]\n",
      " [-143.94241367 -129.04819241  -83.62852136   52.45541413  500.        ]]\n",
      "\n",
      "Iteration 1000\n",
      "[[-1500.         -1799.97137894 -1703.09100168 -1637.47516054\n",
      "  -1617.87512173]\n",
      " [-1784.69761417 -2500.         -1877.73941087 -1731.49417217\n",
      "  -1836.5037816 ]\n",
      " [-1641.93443934 -1783.89068077 -1579.89510492 -1577.19655041\n",
      "  -2500.        ]\n",
      " [-1501.7253931  -1416.87870215 -1083.08416035  -499.71332733\n",
      "    123.94648152]\n",
      " [-1450.10264767 -1300.4903542   -836.18647657   539.48390626\n",
      "   5000.        ]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Policy evaluation\n",
    "\n",
    "delta = []\n",
    "for i in range(numIterations):\n",
    "    copygridWorld = np.copy(gridWorld)\n",
    "    deltaState = []\n",
    "    for state in states:\n",
    "        weightedValue = 0\n",
    "        for action in actions:\n",
    "            finalPosition, reward = actionRewardFunction(state, action)\n",
    "            weightedValue += (1/len(actions))*(reward + gamma*(gridWorld[finalPosition[0], finalPosition[1]]))\n",
    "        deltaState.append(np.abs(copygridWorld[state[0], state[1]] - weightedValue))\n",
    "        copygridWorld[state[0], state[1]] = weightedValue\n",
    "    delta.append(deltaState)\n",
    "    gridWorld = copygridWorld\n",
    "    \n",
    "    if i in [0, 1, 2, 9, 99, numIterations-1]:\n",
    "        print(\"Iteration {}\".format(i+1))\n",
    "        print(gridWorld)\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
